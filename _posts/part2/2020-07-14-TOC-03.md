---
layout: post
title: "Part2 lesson 9, 03_minibatch_training | fastai 2019 course -v3"
author: dionne
categories: [ fastai.v3]
image: assets/images/fastai/03.png
tags: [ feature ]
---


From this notebook, I learned how to re-define our NN *model* and update *training* process.

Original notebook [link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/03_minibatch_training.ipynb)

My replicated Notebook [Link](https://github.com/SpellOnYou/10000-days-of-code/blob/master/fastai_scratch/03_copy.ipynb)

* [Initial Setup](#initial-setup)
{:toc}


## Initial Setup

### Data

1. Fastai for Colab env
2. import data
	1. Hyper parames
		1. (Number of input size = data size
		1. Number of output unit = value
		1. Number of hidden unit = 50
		1. Number of hidden size = 1
	2. Initialize the model ***(1st)***
		1. init with size of nodes
		1. instance call with independent variable(s), returning prediction

### Loss function: Cross entropy loss

1. Softmax
	1. Write down the formula of softmax
	2. Why do we need this softmax function at the last layer?
	3. Write down softmax as code.
	4. Why do I need a log of softmax?

2. Cross Entropy
	1. Write down the formula of cross entropy
	2. Why is this function adequate to the loss of categorical variables?

3. Negative log likelihood function [^1]
	1. Implement the negative log likelihood function 
	2. Why do we find a `negative` value

4. LogSumExp
	1. What is LogSumExp
	2. Why is this dubbed 'trick' somehow?
	3. implement logsumexp function
	4. compare 3 with torch's logsumexp 
	3. re-implement log_softmax function using torch's logsumexp method
	4. compare the loss function with 4 and pre-version

## Basic Training Loop

1. write down the procedure of general training loop (4 steps)
2. define accuracy/loss function.
3. Implement training loop as written above, following
	1. grab one batch
	2. predict results and calculate loss
	3. backward process (calculate gradients)
	4. update parameters with lr, grad

## Using parameters and optimal
### Parameters

> we will use `nn.Module.__setattr__` and move relu to functional. [^3]

1. Re-define Model class using nn.Module. ***(2nd Model)*** (use F.relu / no need to implement backwards)
	1. What’s difference from that you made first?
2. See the layers inside model using `named_children` method
	1. see one layer
3. Re-define `fit` function with using parameters, not layers. ***(2nd Training Loop)***
	1. What’s difference between nn.Module’s layer / parameters?
	1. Why this diff makes shorter code?
4. Make `DummyModule` class to simulate pytorch’s `__setattr__` ***(3rd Model)***
	1. 3 dunder method
		1. Init
		1. setattr
		1. repr
	1. `parameter` funciton generates each every parameters in each layers
	1. What does the setattr do here?
		1. How can i check if I defined parameters properly?
	1. What does the repr do here?
5. Call the instance of dummymodule and see repr and shape of parameters in instance

### Registering modules

1. Re-define Model class ***(4th Model)***
	1. Use original layers approach
	1. Register the modules for every each layer using `add_module`
2. What is `registering modules`?
3. See the model instance (i.e. repr)

### nn.ModuleList
1. Define class SequentialModel ***(5th Model)***
	1. Use nn.ModuleList
2. What does nn.ModuleList do for us?

### nn.Sequential

1. Make model instance using nn.Sequential  ***(6th Model)***
2. Why does this class make jobs easier?

### Optim

1. Define class Optimizer
	1. Init - params, lr
	1. step
		- why do we need torch.no_grad()?
	1. zero_grad
2. Do the one epoch learning with optimizer instance ***(3rd Training Loop)***
3. See the loss and accuracy [^4]
4. Do the one epoch learning using PyTorch’s optim.SGD functionality ***(4th Training Loop)***
	1. Define get_model function
		1. rtype: (1) model instance from nn.Sequential, (2) Optimizer function from optim.SGD
	1. See the loss and accuracy [^5]

## Dataset and DataLoader

We will make a `dataset` / `dataloader` class to make iteration through mini-batches more efficiently.

### Dataset

1. Make class `Dataset` which has three essential methods. (hint: 3)
	1. what are three components?
2. make dataset object 1) get the length and 2) get items using index
	1. compare those tensor's *length / shape* with originals

3. Do the one epoch learning using data from `Datasest` ***(5th training loop)***


### DataLoader

1. make class `DataLoader` which takes dataset and batch size, and instance is iterator returning next batch

2. make `def fit()` which gets data from dataloader class. ***(6th training)***


### Random sampling

1. Why we should shuffle the training set excluding validation set?
2. When to re-shuffle the data? At the beginning of epoch? beginning of minibatch? and why I should?
3. Make `Sampler` class [^7]
	1. initialized with 3 params, (int) dataset len, (int) bs, and (bool) shuffle
	1. make instance as generator which returns [shuffled] index of the next batch

4. Test class Sampler with 10 size of data, and 3 of bs [^6], [^8]
	1. suppose this data is train data and see the returnings
	1. suppose this data is valid data and see the returnings
5. re-defin `DataLoader` (2nd)
	1. initialized with
		1. dataset instance
		2. sampler instance
		3. collate function: gets randomly extracted x, y pair and stack them
	1. generates stacked tensor
	1. What do we need collate function? How can I variate them?
6. test newly defined dataloader
	1. grab one batch and plot the first item
	1. grab next batch and plot the first item
	1. see the loss and accuracy with newly clutched batch [^10]

### PyTorch DataLoader

1. What's difference between pytorch's dataloader and our dataloader?
2. from torch, call dataloader, sampler for train and validation
3. see the loss and accuracy
	- rendering collate and sampler
	- do above same way without previously defined collate and sampler [^11]

## Validation

1. make the new fit function (7th training)
	1. when should we validate?
	2. why we should have a validation set? How can we judge overfitting happened using validation set?
	3. why should we call train/valid? How does valid works here? (use word 'inference')
3. Answer to the Jeremy's Question: *Are these validation results correct if batch size varies?*
4. make get_dls funciton
	1. return: dataloaders for train/valid sets
	2. why validation set has two times bigger batch size?
	3. there's no optimization phase when to validate the model, then why do we need `with torch.no_grad():`?

5. implement in 3 lines of code which obtain the data loader and fit the model 

## Notes

[^1]: I’ve heard that finding convex downward / upward is not the same and there are preferences. Find out which is easier and explain why.
[^2]: What’s relationship with above one epoch training and this one batch loss / accuracy?
[^3]: Study other dunders
[^4]: I don’t know if I’m getting better loss since I’m using better approach or doing same thing at same data repeatedly -> shallow: check I would get same result when I run this cell before others deep: inspect details of mechanisms each approach uses
[^5]: What does it mean ‘except we’ll be doing it in a more flexible way!’?
[^6]: why is this returning its own?
[^7]: torch.randperm and torch.arange
[^8]: pack and unpack the argument of python
[^9]: Why do we need to dataset and sampler instance which was already given dataset as argument?
[^10]: Why did he renew the model when to test new dataset?
[^11]: why is loss of the second less?
